# Azure Data Factory Project: Automated Data Fetching & Conditional Data Copy Pipelines ğŸš€

## ğŸ“ˆ Project Overview

This project demonstrates automated data ingestion, transformation, monitoring, and notification pipelines built using **Azure Data Factory (ADF)**. The core focus areas are:

1. ğŸŒ **REST API Data Fetching & Storage**
2. ğŸ“¢ **Database to Data Lake Conditional Data Copy with Child Pipeline Triggering**
3. ğŸ“ˆ **Pipeline Monitoring Dashboard using Power BI**

Additional advanced features include:

* â° Scheduled triggers
* ğŸ“Š Parameter passing between pipelines
* ğŸ“© Gmail notifications for operational monitoring
* ğŸ“ Modular and scalable design
* ğŸ“‰ Visual monitoring using Power BI

---

## âœ… Tasks Breakdown

### ğŸ“Œ **Task 1: Fetch Country Data using REST API and Save as JSON**

* **Objective:** Automatically fetch data for five countries via REST API and store as JSON files in Azure Data Lake Storage (ADLS).
* **Countries Handled:**
  - India
  - US
  - UK
  - China
  - Russia

* **Details:**
  * API Endpoint: https://restcountries.com/v3.1/name/{country_name}
  * Loop through country list dynamically.
  * Save each countryâ€™s JSON response as `{country_name}.json` file in ADLS.

---

### ğŸ“Œ **Task 2: Create Automated Trigger for Task 1**

* **Objective:** Schedule country data fetching pipeline to run twice daily.
* **Trigger Configuration:**
  * â° 12:00 AM IST
  * â° 12:00 PM IST
* **Benefits:**
  * Eliminates manual execution.
  * Maintains up-to-date country data.

---

### ğŸ“Œ **Task 3: Conditional Copy of Customer Data from Database to ADLS**

* **Objective:** Copy customer data only when record count exceeds 500.
* **Steps:**
  * Query database to fetch record count.
  * Use If Condition to check: count > 500.
  * If True, copy data from SQL Database to ADLS.
  * Upon success, trigger child pipeline.

---

### ğŸ“Œ **Task 4: Call Child Product Pipeline if Condition is Met**

* **Objective:** Execute Product Data Pipeline when customer record count exceeds 600.
* **Details:**
  * Pass customer count as a pipeline parameter to child pipeline.
  * Child pipeline handles product data copy based on this parameter.

---

### ğŸ“Œ **Additional Features 1: Gmail Notification for Status Update (Optional)**

* **Objective:** Notify via Gmail upon success or failure of pipelines.
* **Implementation:**
  * Gmail API / Logic Apps used for notifications.
  * Helps in proactive pipeline monitoring.

---

### ğŸ“Œ **Additional Features 2: Monitoring Dashboard using Power BI (Optional)**

* **Objective:** Visualize pipeline runs, failures, and data ingestion trends using a Power BI dashboard connected to SQL Server (SSMS).

* **Steps:**
  1. Store pipeline audit logs into SQL Server audit table.
  2. Connect Power BI to SQL Server using SQL Database connector.
  3. Build a monitoring dashboard showing:
     * ğŸ“Š Total Pipeline Runs (KPI Card)
     * âœ… Successful Runs (KPI Card)
     * âŒ Failed Runs (KPI Card)
     * ğŸ“‰ Data Ingested per Day (Bar/Line Chart)
     * ğŸ¥§ Success vs. Failure Distribution (Pie Chart)
     * âš ï¸ Error Logs (Table View)

* **Benefits:**
  * Centralized monitoring and performance insights.
  * Real-time visibility of pipeline health and data flows.
  * Proactive failure detection using visual dashboards.

---

## ğŸ“‚ Project Folder Structure

```
â”œâ”€â”€ Task 1: CountryData_Fetch_Pipeline ğŸŒ
â”œâ”€â”€ Task 2: Triggers â°
â”œâ”€â”€ Task 3: Customer_Data_Conditional_Copy_Pipeline ğŸ“¢
â”œâ”€â”€ Task 4: Product_Data_Child_Pipeline ğŸ“¦
â”œâ”€â”€ Task 5: Gmail_Notification_Pipeline ğŸ“©
â”œâ”€â”€ Task 6: PowerBI_Monitoring_Dashboard ğŸ“Š
â”œâ”€â”€ linked_services/
â”œâ”€â”€ datasets/
â””â”€â”€ monitoring_reports/
```

---

## ğŸ’¡ Key Features ğŸŒŸ

* ğŸŒ REST API Data Ingestion  
* ğŸ“Š Parameterized Pipeline Execution  
* âœ… Condition-Based Data Copy  
* ğŸ“‚ Dynamic File Naming  
* â° Scheduled Trigger Automation  
* ğŸ“© Email Notifications via Gmail  
* ğŸ“¦ Modular & Scalable Pipeline Design  
* âš™ï¸ Robust Monitoring & Error Handling  
* ğŸ“¬ Real-time Gmail-based Status Alerts  
* ğŸ“Š **Power BI Monitoring Dashboard for Centralized Reporting**

---

## ğŸ“– How to Deploy ğŸ› ï¸

1. ğŸ” Import all pipelines, linked services, and datasets into Azure Data Factory instance.
2. ğŸ”§ Configure REST API endpoints, database connections, and ADLS paths.
3. â° Set triggers for automated scheduling.
4. ğŸ“© Deploy notification logic via Gmail API or Logic Apps.
5. ğŸ“Š Connect Power BI to SQL Server audit table and publish monitoring reports.
6. ğŸŒŸ Monitor pipeline executions via ADF Monitoring panel and Power BI Dashboard.

---

## ğŸ“§ Author ğŸ“‡

**Himanshu Ranjan**  
*Azure Data Engineer Intern | SQL | Azure | ADF | Python*  
ğŸ”— [Connect on LinkedIn](https://www.linkedin.com/in/himanshu-ranjan-88524b273/)  
ğŸ“¨ [krithimanshu4321@gmail.com](mailto:krithimanshu4321@gmail.com)

---

## ğŸ“™ License ğŸ“œ

This project is open-source and available under the MIT License.

---

ğŸ“¬ **Contact**  
For any queries, collaboration, or demo, reach out at: [krithimanshu4321@gmail.com](mailto:krithimanshu4321@gmail.com)

â€œEmpowering insights through seamless data automation and visualization.â€

---

Happy Data Automation! ğŸš€ğŸ‰


